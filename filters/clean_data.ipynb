{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to clean JSON and csv inputs for better outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "from difflib import get_close_matches # for fuzzy matching\n",
    "from collections import defaultdict\n",
    "import instaloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the cleaned restaurants data\n",
    "restaurants_df = pd.read_csv('../outputs/cleaned_restaurants.csv')\n",
    "\n",
    "# Create priority for A/B/C grades\n",
    "grade_priority = {'A': 1, 'B': 2, 'C': 3}\n",
    "restaurants_df['grade_priority'] = restaurants_df['GRADE'].map(grade_priority).fillna(4)\n",
    "\n",
    "# Sort by DBA and grade priority\n",
    "restaurants_df = restaurants_df.sort_values(by=['DBA', 'grade_priority'])\n",
    "\n",
    "# Remove duplicates, keeping first valid grade\n",
    "restaurants_df = restaurants_df.drop_duplicates(subset=['DBA'], keep='first')\n",
    "\n",
    "# If grade wasn't A/B/C, set to blank\n",
    "restaurants_df.loc[~restaurants_df['GRADE'].isin(['A', 'B', 'C']), 'GRADE'] = ''\n",
    "\n",
    "# Read the borough cuisines data\n",
    "borough_df = pd.read_csv('../outputs/nyc_borough_cuisines.csv')\n",
    "\n",
    "# Convert to lowercase for case-insensitive matching\n",
    "restaurants_df['DBA_lower'] = restaurants_df['DBA'].str.lower()\n",
    "borough_df['name_lower'] = borough_df['name'].str.lower()\n",
    "\n",
    "# Merge with case-insensitive matching\n",
    "merged_df = pd.merge(\n",
    "    restaurants_df[['DBA', 'BORO', 'GRADE', 'DBA_lower']],\n",
    "    borough_df[['name', 'rating', 'review_count', 'categories_list', 'name_lower']],\n",
    "    left_on='DBA_lower',\n",
    "    right_on='name_lower',\n",
    "    how='inner'\n",
    ").drop_duplicates(subset=['DBA_lower'])\n",
    "\n",
    "# Rename columns and select final columns\n",
    "merged_df = merged_df.rename(columns={\n",
    "    'DBA': 'name',\n",
    "    'BORO': 'borough'\n",
    "})[['name', 'borough', 'rating', 'review_count', 'categories_list', 'GRADE']]\n",
    "\n",
    "# Save to final merged file\n",
    "merged_df.to_csv('../outputs/merged_restaurants_final.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurant names saved to ../outputs/restaurant_names.txt\n"
     ]
    }
   ],
   "source": [
    "# get restaurant names from inspection data\n",
    "try:\n",
    "        df = pd.read_csv(\"../outputs/merged_restaurant_data.csv\")\n",
    "\n",
    "        # Extract the 'name' column and add \"NYC restaurant\"\n",
    "        restaurant_names = df['name'].tolist()  # Convert to a list\n",
    "        formatted_names = [f\"{name} NYC restaurant\" for name in restaurant_names]\n",
    "\n",
    "\n",
    "        # Write names to text file\n",
    "        with open(\"../outputs/restaurant_names.txt\", 'w', encoding='utf-8') as txtfile: # Use encoding to support non-ascii\n",
    "            for name in formatted_names:\n",
    "                txtfile.write(name + '\\n') # newline after each name\n",
    "\n",
    "        print(f\"Restaurant names saved to {\"../outputs/restaurant_names.txt\"}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input CSV file '{\"../outputs/merged_restaurant_data.csv\"}' not found.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Column '{e}' not found in the CSV. Check the header.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to file\n"
     ]
    }
   ],
   "source": [
    "# add instagram data restaurant names\n",
    "\n",
    "with open(\"../outputs/IG_posts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    insta_data = json.load(f)\n",
    "    \n",
    "\n",
    "restaurant_names = set()\n",
    "\n",
    "for entry in insta_data:\n",
    "    caption = entry.get(\"caption\", \"\").lower()\n",
    "    \n",
    "    # Extract restaurant names from caption\n",
    "    possible_names = re.findall(r'@([a-zA-Z0-9_]+)', caption)\n",
    "    restaurant_names.update(possible_names)\n",
    "\n",
    "restaurant_names = list(restaurant_names)\n",
    "\n",
    "# put into file\n",
    "\n",
    "with open(\"../outputs/restaurant_names.json\", \"w\") as file:\n",
    "    \n",
    "    json.dump(restaurant_names, file, indent=4)\n",
    "    \n",
    "    print(\"Data has been written to file\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes CSV saved to ../outputs/nodes.csv\n"
     ]
    }
   ],
   "source": [
    "# create nodes for Gephi\n",
    "\n",
    "\"\"\"Creates a Gephi-compatible nodes CSV from the restaurant data.\"\"\"\n",
    "try:\n",
    "    df = pd.read_csv(\"../outputs/merged_restaurant_data.csv\")\n",
    "\n",
    "    # Select desired columns and rename \"DBA\" to \"Label\" for Gephi compatibility\n",
    "    nodes_df = df[[\"name\", \"BOROUGH\", \"categories_list\", \"SCORE\", \"GRADE\"]].copy()  # Create a copy to avoid warnings\n",
    "\n",
    "    nodes_df.rename(columns={\"name\": \"Label\"}, inplace=True) #Rename here\n",
    "\n",
    "    # Add an \"Id\" column (essential for Gephi) - use index as ID\n",
    "    nodes_df.insert(0, 'Id', range(len(nodes_df)))  # Insert at beginning\n",
    "\n",
    "\n",
    "    # Ensure 'SCORE' is numeric and handle errors gracefully\n",
    "    nodes_df['SCORE'] = pd.to_numeric(nodes_df['SCORE'], errors='coerce') # Convert to numeric, invalid values become NaN\n",
    "\n",
    "\n",
    "    # Handle potential empty or NaN values (replace with empty string to avoid Gephi import issues)\n",
    "    nodes_df.fillna('', inplace=True) #Important to use fillna after to_numeric.  Otherwise you replace valid numbers\n",
    "\n",
    "\n",
    "    # Write nodes to CSV\n",
    "    nodes_df.to_csv(\"../outputs/nodes.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"Nodes CSV saved to {\"../outputs/nodes.csv\"}\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input CSV file not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create edges for gephi\n",
    "\n",
    "df = pd.read_csv(\"../outputs/nodes.csv\")\n",
    "edges = []\n",
    "for i in range(len(df)):\n",
    "    for j in range(i + 1, len(df)):\n",
    "        if df.iloc[i]['BOROUGH'] == df.iloc[j]['BOROUGH']:\n",
    "            edges.append([df.iloc[i]['Id'], df.iloc[j]['Id'], \"Undirected\", 1])  # Create edge\n",
    "\n",
    "\n",
    "edges_df = pd.DataFrame(edges, columns=[\"Source\", \"Target\", \"Type\", \"Weight\"])\n",
    "edges_df.to_csv(\"../outputs/boro_edges.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to file\n"
     ]
    }
   ],
   "source": [
    "# filter google places reviews\n",
    "\n",
    "with open(\"../outputs/review_aggregated.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    review_data = json.load(f)\n",
    "    \n",
    "filtered_reviews = []\n",
    "\n",
    "for entry in review_data:\n",
    "    \n",
    "    updates = entry.get(\"updatesFromCustomers\", {})\n",
    "    posted_by = updates.get(\"postedBy\", {}) if updates else {}\n",
    "    \n",
    "    filtered_review = {\n",
    "        \"city\": entry.get(\"city\"),\n",
    "        \"category\": entry.get(\"categoryName\"),\n",
    "        \"title\": entry.get(\"title\"),\n",
    "        \"score\" : entry.get(\"totalScore\"),\n",
    "        \"review\": updates.get(\"text\") if updates else None,\n",
    "        \"name\" : posted_by.get(\"name\") if posted_by else None,\n",
    "        \"links\" : [media.get(\"link\") for media in updates.get(\"media\", [])] if updates else []\n",
    "    }\n",
    "    \n",
    "    filtered_reviews.append(filtered_review)\n",
    "\n",
    "with open(\"../outputs/filtered_reviews.json\", \"w\") as file:\n",
    "    json.dump(filtered_reviews, file, indent=4)\n",
    "    \n",
    "    print(\"Data has been written to file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep those review names which we have active information for\n",
    "with open(\"../outputs/restaurant_names.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    restaurant_names = [line.strip().replace(\" NYC restaurant\", \"\") for line in f.readlines()]\n",
    "    \n",
    "with open(\"../outputs/filtered_reviews.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    filtered_data = json.load(f)\n",
    "    \n",
    "filtered_data = [entry for entry in filtered_data if entry.get(\"title\") and get_close_matches(entry.get(\"title\"), restaurant_names, n=1, cutoff=0.6)]\n",
    "\n",
    "with open(\"../outputs/filtered_reviews.json\", \"w\", encoding = \"utf-8\") as file:\n",
    "    json.dump(filtered_data, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump those restaurants into a .txt file \n",
    "\n",
    "with open(\"../outputs/filtered_reviews.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in filtered_data:\n",
    "        f.write(entry.get(\"title\") + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered and decoded data saved. 3779 entries remaining.\n"
     ]
    }
   ],
   "source": [
    "# remove blank fields (where restaurant_name = \"\") in the instagram posts\n",
    "\n",
    "# this is to be run after extract_restaurant_names.py\n",
    "# Load data from the JSON file\n",
    "with open(\"../outputs/IG_posts.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Remove entries where restaurantName is empty\n",
    "filtered_data = [entry for entry in data if entry.get(\"restaurantName\", \"\").strip() != \"\"]\n",
    "\n",
    "# Save the filtered data back to the same file (or change file_path to save as a new file)\n",
    "with open(\"../outputs/IG_posts.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(filtered_data, file, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Filtered data saved. {len(filtered_data)} entries remaining.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab popular influencers using instaloader\n",
    "\n",
    "# Initialize Instaloader\n",
    "L = instaloader.Instaloader()\n",
    "\n",
    "# Load posts from the JSON file\n",
    "\n",
    "with open('../outputs/IG_posts.json', 'r') as file:\n",
    "    posts = json.load(file)\n",
    "\n",
    "influencers = {}\n",
    "\n",
    "# Process each post\n",
    "for post in posts:\n",
    "    username = post.get('influencerUsername')\n",
    "    restaurant_name = post.get('restaurantName')\n",
    "    \n",
    "    # Avoid processing duplicates\n",
    "    if username in influencers:\n",
    "        continue\n",
    "    \n",
    "    print(f\"Checking influencer: {username}\")\n",
    "    try:\n",
    "        profile = instaloader.Profile.from_username(L.context, username)\n",
    "        follower_count = profile.followers\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving profile for {username}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"{username} has {follower_count} followers.\")\n",
    "    if follower_count > follower_threshold:\n",
    "        influencers[username] = {\n",
    "            \"influencerUsername\": username,\n",
    "            \"restaurantName\": restaurant_name,\n",
    "            \"followers\": follower_count  # Optional: add the followers count for reference.\n",
    "        }\n",
    "        print(f\"Added {username} to the list of NYC food influencers.\")\n",
    "    else:\n",
    "        print(f\"Skipped {username} as follower count is below threshold.\")\n",
    "\n",
    "# Write the qualifying influencers to the output JSON file\n",
    "\n",
    "with open(\"../outputs/nyc_food_influencers.json\", 'w') as out_file:\n",
    "    json.dump(list(influencers.values()), out_file, indent=2, ensure_ascii=False)\n",
    "print(f\"\\nSaved {len(influencers)} influencers to {output_file}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Unicode characters have been converted to their respective symbols.\n"
     ]
    }
   ],
   "source": [
    "# deal with unicode error\n",
    "\n",
    "with open(\"../outputs/IG_posts.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Save it back using actual symbols (not \\u escape sequences)\n",
    "with open(\"../outputs/IG_posts.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(data, file, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"All Unicode characters have been converted to their respective symbols.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All missing 'ratingAvailable' fields filled where appropriate.\n"
     ]
    }
   ],
   "source": [
    "# auxillary function to reconcile if same name -> assign ratingAvailable: true\n",
    "# run periodically as I was going through IG_posts.json by hand\n",
    "\n",
    "with open(\"../outputs/IG_posts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Step 1: Identify restaurantNames where any entry has ratingAvailable == true\n",
    "restaurants_with_rating = set(\n",
    "    entry[\"restaurantName\"]\n",
    "    for entry in data\n",
    "    if entry.get(\"ratingAvailable\") is True\n",
    ")\n",
    "\n",
    "# Step 2: Set ratingAvailable = True for all matching restaurantNames where it's missing\n",
    "for entry in data:\n",
    "    if (\n",
    "        entry.get(\"restaurantName\") in restaurants_with_rating and\n",
    "        \"ratingAvailable\" not in entry\n",
    "    ):\n",
    "        entry[\"ratingAvailable\"] = True\n",
    "\n",
    "# Step 3: Save back to the file\n",
    "with open(\"../outputs/IG_posts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ All missing 'ratingAvailable' fields filled where appropriate.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
