{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to clean JSON and csv inputs for better outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "from difflib import get_close_matches # for fuzzy matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurant names saved to ../outputs/restaurant_names.txt\n"
     ]
    }
   ],
   "source": [
    "# get restaurant names from inspection data\n",
    "try:\n",
    "        df = pd.read_csv(\"../outputs/merged_restaurant_data.csv\")\n",
    "\n",
    "        # Extract the 'name' column and add \"NYC restaurant\"\n",
    "        restaurant_names = df['name'].tolist()  # Convert to a list\n",
    "        formatted_names = [f\"{name} NYC restaurant\" for name in restaurant_names]\n",
    "\n",
    "\n",
    "        # Write names to text file\n",
    "        with open(\"../outputs/restaurant_names.txt\", 'w', encoding='utf-8') as txtfile: # Use encoding to support non-ascii\n",
    "            for name in formatted_names:\n",
    "                txtfile.write(name + '\\n') # newline after each name\n",
    "\n",
    "        print(f\"Restaurant names saved to {\"../outputs/restaurant_names.txt\"}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input CSV file '{\"../outputs/merged_restaurant_data.csv\"}' not found.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Column '{e}' not found in the CSV. Check the header.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to file\n"
     ]
    }
   ],
   "source": [
    "# add instagram data restaurant names\n",
    "\n",
    "with open(\"../outputs/IG_posts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    insta_data = json.load(f)\n",
    "    \n",
    "\n",
    "restaurant_names = set()\n",
    "\n",
    "for entry in insta_data:\n",
    "    caption = entry.get(\"caption\", \"\").lower()\n",
    "    \n",
    "    # Extract restaurant names from caption\n",
    "    possible_names = re.findall(r'@([a-zA-Z0-9_]+)', caption)\n",
    "    restaurant_names.update(possible_names)\n",
    "\n",
    "restaurant_names = list(restaurant_names)\n",
    "\n",
    "# put into file\n",
    "\n",
    "with open(\"../outputs/restaurant_names.json\", \"w\") as file:\n",
    "    \n",
    "    json.dump(restaurant_names, file, indent=4)\n",
    "    \n",
    "    print(\"Data has been written to file\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes CSV saved to ../outputs/nodes.csv\n"
     ]
    }
   ],
   "source": [
    "# create nodes for Gephi\n",
    "\n",
    "\"\"\"Creates a Gephi-compatible nodes CSV from the restaurant data.\"\"\"\n",
    "try:\n",
    "    df = pd.read_csv(\"../outputs/merged_restaurant_data.csv\")\n",
    "\n",
    "    # Select desired columns and rename \"DBA\" to \"Label\" for Gephi compatibility\n",
    "    nodes_df = df[[\"name\", \"BOROUGH\", \"categories_list\", \"SCORE\", \"GRADE\"]].copy()  # Create a copy to avoid warnings\n",
    "\n",
    "    nodes_df.rename(columns={\"name\": \"Label\"}, inplace=True) #Rename here\n",
    "\n",
    "    # Add an \"Id\" column (essential for Gephi) - use index as ID\n",
    "    nodes_df.insert(0, 'Id', range(len(nodes_df)))  # Insert at beginning\n",
    "\n",
    "\n",
    "    # Ensure 'SCORE' is numeric and handle errors gracefully\n",
    "    nodes_df['SCORE'] = pd.to_numeric(nodes_df['SCORE'], errors='coerce') # Convert to numeric, invalid values become NaN\n",
    "\n",
    "\n",
    "    # Handle potential empty or NaN values (replace with empty string to avoid Gephi import issues)\n",
    "    nodes_df.fillna('', inplace=True) #Important to use fillna after to_numeric.  Otherwise you replace valid numbers\n",
    "\n",
    "\n",
    "    # Write nodes to CSV\n",
    "    nodes_df.to_csv(\"../outputs/nodes.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"Nodes CSV saved to {\"../outputs/nodes.csv\"}\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input CSV file not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create edges for gephi\n",
    "\n",
    "df = pd.read_csv(\"../outputs/nodes.csv\")\n",
    "edges = []\n",
    "for i in range(len(df)):\n",
    "    for j in range(i + 1, len(df)):\n",
    "        if df.iloc[i]['BOROUGH'] == df.iloc[j]['BOROUGH']:\n",
    "            edges.append([df.iloc[i]['Id'], df.iloc[j]['Id'], \"Undirected\", 1])  # Create edge\n",
    "\n",
    "\n",
    "edges_df = pd.DataFrame(edges, columns=[\"Source\", \"Target\", \"Type\", \"Weight\"])\n",
    "edges_df.to_csv(\"../outputs/boro_edges.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to file\n"
     ]
    }
   ],
   "source": [
    "# filter google places reviews\n",
    "\n",
    "with open(\"../outputs/review_aggregated.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    review_data = json.load(f)\n",
    "    \n",
    "filtered_reviews = []\n",
    "\n",
    "for entry in review_data:\n",
    "    \n",
    "    updates = entry.get(\"updatesFromCustomers\", {})\n",
    "    posted_by = updates.get(\"postedBy\", {}) if updates else {}\n",
    "    \n",
    "    filtered_review = {\n",
    "        \"city\": entry.get(\"city\"),\n",
    "        \"category\": entry.get(\"categoryName\"),\n",
    "        \"title\": entry.get(\"title\"),\n",
    "        \"score\" : entry.get(\"totalScore\"),\n",
    "        \"review\": updates.get(\"text\") if updates else None,\n",
    "        \"name\" : posted_by.get(\"name\") if posted_by else None,\n",
    "        \"links\" : [media.get(\"link\") for media in updates.get(\"media\", [])] if updates else []\n",
    "    }\n",
    "    \n",
    "    filtered_reviews.append(filtered_review)\n",
    "\n",
    "with open(\"../outputs/filtered_reviews.json\", \"w\") as file:\n",
    "    json.dump(filtered_reviews, file, indent=4)\n",
    "    \n",
    "    print(\"Data has been written to file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep those review names which we have active information for\n",
    "with open(\"../outputs/restaurant_names.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    restaurant_names = [line.strip().replace(\" NYC restaurant\", \"\") for line in f.readlines()]\n",
    "    \n",
    "with open(\"../outputs/filtered_reviews.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    filtered_data = json.load(f)\n",
    "    \n",
    "filtered_data = [entry for entry in filtered_data if entry.get(\"title\") and get_close_matches(entry.get(\"title\"), restaurant_names, n=1, cutoff=0.6)]\n",
    "\n",
    "with open(\"../outputs/filtered_reviews.json\", \"w\", encoding = \"utf-8\") as file:\n",
    "    json.dump(filtered_data, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump those restaurants into a .txt file \n",
    "\n",
    "with open(\"../outputs/filtered_reviews.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in filtered_data:\n",
    "        f.write(entry.get(\"title\") + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
